kafka {
  application.id = "daisy.dev.candyfloss"
  group.id = "daisy.dev.candyfloss-1"
  bootstrap.servers = "kafka.sbd.corproot.net:9093"
  acks = 1
  linger.ms = 5
  processing.guarantee = at_least_once
  enable.idempotence = false
  schema.registry.url = "https://schema-registry.zhh.sbd.corproot.net"
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  metrics.recording.level = DEBUG
  compression.type = zstd
  replication.factor = 2
  num.stream.threads = 2
  auto.offset.reset = latest
  probing.rebalance.interval.ms = 120000
}
kstream {
  input.topic.name = daisy.dev.device-json-raw
  discard.topic.name = daisy.dev.candyfloss-discard
  dlq.topic.name = daisy.dev.candyfloss-dlq
  state.store.name = candyfloss-counters-store
  state.store.max.counter.cache.age = 900000 // 15 minutes
  state.store.int.counter.wrap.limit = 10000
  state.store.long.counter.wrap.limit = 10000000
  state.store.long.counter.time.ms = 300000 # allow max 5 min for counter wrap around otherwise it's a reset
  state.store.delete.scan.frequency.days = 7 # how often to trigger scanning for old unused counters and delete them
  pre.transform = pre-transform.ietf.dev.json
  pipeline = {
    # todo: customize ietf-telemetry-message pipeline here
    fano-interfaces {
      output.topic.name = output-topic-ietf-telemetry-message
      file = pipeline-ietf-telemetry-message/fano-interfaces.json
    }
  }
}
