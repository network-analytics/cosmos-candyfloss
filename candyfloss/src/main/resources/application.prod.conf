kafka {
  # Kafka and Kafka streams paramaerts, check https://docs.confluent.io/platform/current/installation/configuration/streams-configs.html for more details
  application.id = "candyfloss-prod-app-id"
  group.id = "cosmos.prod.candyflows-1"
  bootstrap.servers = "localhost:9092"
  acks = all
  enable.idempotence = true
  schema.registry.url = "https://localhost:8081"
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  metrics.recording.level = DEBUG
  compression.type = gzip
  replication.factor = 2
  num.stream.threads = 2
  probing.rebalance.interval.ms = 120000
}
kstream {
  input.topic.name = prod.device-json-raw-input
  discard.topic.name = prod.candyfloss-processing-discard # All messages that didn't match any step in the pipeline
  dlq.topic.name = prod.candyfloss-processing-dlq # Messages that encountered a Java exception during the processing
  state.store.name = prod.candyfloss-counters-store # Kafka state store name to save the counter values
  state.store.max.counter.cache.age = 900000 // 15 minutes
  state.store.int.counter.wrap.limit = 10000
  state.store.long.counter.wrap.limit = 10000000
  state.store.long.counter.time.ms = 300000 # allow max 5 min for counter wrap around otherwise it's a reset
  state.store.delete.scan.frequency.days = 7 # how often to trigger scanning for old unused counters and delete them
  pre.transform = pre-transform.prod.json # A pre transformation step that applies to all messages
  pipeline = {
    oc-interface {
      output.topic.name = device-oc-int-proc-json
      file = pipeline/oc-interface.json
    }
  }
}
