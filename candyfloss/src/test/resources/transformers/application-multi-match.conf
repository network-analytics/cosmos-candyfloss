kafka {
  application.id = "kafka-streams-101"
  bootstrap.servers = "localhost:29092"
  acks = all
  enable.idempotence = true
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  metrics.recording.level = DEBUG
}

kstream {
  input.topic.name = daisy.prod.device-json-raw
  discard.topic.name = daisy.dev.device-json-flat
  dlq.topic.name = daisy.dev.device-json-flat-dlq
  normalization.statestore.cutoff.time.ms = 1800000 # 30 minutes
  state.store.name = daisy.dev.device-counters-store
  state.store.max.counter.cache.age = 1000
  state.store.int.counter.wrap.limit = 10000
  state.store.long.counter.wrap.limit = 10000000
  state.store.long.counter.time.ms = 300000 # allow max 5 min for counter wrap around otherwise it's a reset
  state.store.delete.scan.frequency.days = 7 # how often to trigger scanning for old unused counters and delete them
  pipeline = {
    output1 {
      output.topic.name = output1
      file = transformers/pipeline/output1.json
    }
    outputX {
      output.topic.name = outputX
      file = transformers/pipeline/pipelineX.json
    }
    output2 {
      output.topic.name = output2
      file = transformers/pipeline/output2.json
    }
    outputY {
      output.topic.name = outputY
      file = transformers/pipeline/pipelineY.json
    }
    output3 {
      output.topic.name = output3
      file = transformers/pipeline/output3.json
    }
  }
}
