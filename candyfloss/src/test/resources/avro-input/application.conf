
kafka {
  application.id = "kafka-streams-101"
  bootstrap.servers = "localhost:29092"
  acks = all
  enable.idempotence = true
  key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
  value.serializer = "io.confluent.kafka.streams.serdes.avro.GenericAvroSerde"
  schema.registry.url = "mock://test"
  metrics.recording.level = DEBUG
}

kstream {
  input.topic.name = daisy.dev.avro-input
  input.type = "AVRO"
  discard.topic.name = daisy.dev.avro-input-discard
  dlq.topic.name =daisy.dev.avro-input-dlq
  state.store.name = daisy.dev.avro-input-store
  normalization.statestore.cutoff.time.ms = 1800000 # 30 minutes
  state.store.max.counter.cache.age = 1000
  state.store.int.counter.wrap.limit = 10000
  state.store.long.counter.wrap.limit = 10000000
  state.store.long.counter.time.ms = 300000 # allow max 5 min for counter wrap around otherwise it's a reset
  state.store.delete.scan.frequency.days = 7 # how often to trigger scanning for old unused counters and delete them
  pipeline = {
    output1 {
      output.topic.name = output1
      file = avro-input/pipeline.json
    }
  }
}
